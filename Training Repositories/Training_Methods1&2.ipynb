{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_Methods1&2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Yav3lKMmgcmtvVRH0KxdqUsTYfm5Pdu1",
      "authorship_tag": "ABX9TyPSonG+GJKk6cFWoyhoEpBm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raulbenitez/CHESS_ROBOT/blob/master/Training%20Repositories/Training_Methods1%262.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS14mbDdqF5U",
        "colab_type": "text"
      },
      "source": [
        "#Set enviornment and networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jmWybPsoyVM",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries and datasets:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSVez5vSjiGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import cv2\n",
        "\n",
        "#Input Keras and Tensorflow Environment\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, utils, regularizers, applications, optimizers\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "print(\"\\nEnviornments imported\")\n",
        "folder = './dataset/'\n",
        "!unzip -q \"./drive/My Drive/dataset-v3.zip\" -d \"dataset\"\n",
        "\n",
        "print(\"\\nDataset loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C60jMgaSpEp1",
        "colab_type": "text"
      },
      "source": [
        "**Build CNN Structure:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkMfLqFgY9zE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"1. Create model: \\n\")\n",
        "\n",
        "nclasses = 13\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(nclasses,activation='softmax'))\n",
        "\n",
        "episodes = 100   \n",
        "learning_rate = 0.001\n",
        "\n",
        "opt = optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "checkpoint_path = \"ckpoint_cnn_model/cnn_model.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "model.save('cnn_model.h5')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vuj7lbupQEt",
        "colab_type": "text"
      },
      "source": [
        "**Load pre-trained VGG19 and add top layers:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1iXIsFc8Z31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(\"1. Create model: \\n\")\n",
        "\n",
        "print(\"Loading pretrained VGG19 model... \\n\")\n",
        "\n",
        "base_model = applications.vgg19.VGG19(weights='imagenet',include_top=False)\n",
        "model = models.Sequential()\n",
        "\n",
        "x = base_model.output\n",
        "\n",
        "print(\"Model loaded\")\n",
        "\n",
        "print(\"Add trainable layers for fine tunning: \")\n",
        "\n",
        "nclasses = 13\n",
        "\n",
        "x=layers.GlobalAveragePooling2D()(x)\n",
        "x=layers.Dense(1024,activation='relu')(x)             #Dense layer 1\n",
        "x=layers.Dropout(0.5)(x)\n",
        "x=layers.Dense(1024,activation='relu')(x)             #Dense layer 2\n",
        "x=layers.Dropout(0.4)(x)\n",
        "preds=layers.Dense(nclasses,activation='softmax')(x)  #Final layer with softmax activation\n",
        "\n",
        "model = models.Model(inputs=base_model.input,outputs=preds)\n",
        "\n",
        "\n",
        "#Freeze base layers\n",
        "for layer in model.layers[:-9]:\n",
        "    layer.model=False\n",
        "for layer in model.layers[-9:]:\n",
        "    layer.trainable=True\n",
        "    \n",
        "episodes = 100    \n",
        "learning_rate = 0.001\n",
        "\n",
        "checkpoint_path = \"ckpoint_vggmodel/vgg19_model.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "opt = optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.save('vggmodel19.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG7egY_zpexj",
        "colab_type": "text"
      },
      "source": [
        "**Load Existing Network:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE4bEuTqfOVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder_model = 'vgg19_dataset1.h5'\n",
        "model = models.load_model(folder_model)\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "    \n",
        "episodes = 100   \n",
        "learning_rate = 0.001\n",
        "\n",
        "opt = optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "checkpoint_path = \"ckpoint_vggmodel/vgg19_model.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "print(\"\\n\",len(model.layers))\n",
        "model.save('vgg19_tfm.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFsvNAvziCHQ",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY-xpZB6pr2v",
        "colab_type": "text"
      },
      "source": [
        "**Load dataset and perform Data Augmentation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFqLnITKp4wt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"2. Create dataset:\\n\")\n",
        "\n",
        "train_folder = folder + 'train'\n",
        "test_folder = folder + 'validate'\n",
        "\n",
        "print(train_folder)\n",
        "print(test_folder)\n",
        "\n",
        "datagen = image.ImageDataGenerator(\n",
        "                                rescale=1./255,\n",
        "                                rotation_range=90,\n",
        "                                width_shift_range=0.00,\n",
        "                                height_shift_range=0.00,\n",
        "                                brightness_range=[0.7,1.0],\n",
        "                                horizontal_flip=True)\n",
        "\n",
        "train = datagen.flow_from_directory(\n",
        "                                directory=train_folder,\n",
        "                                target_size=(224, 224),\n",
        "                                color_mode=\"rgb\",\n",
        "                                batch_size=64,\n",
        "                                class_mode=\"sparse\",\n",
        "                                shuffle=True)\n",
        "\n",
        "validate = datagen.flow_from_directory(\n",
        "                                directory=test_folder,\n",
        "                                target_size=(224, 224),\n",
        "                                color_mode=\"rgb\",\n",
        "                                batch_size=64,\n",
        "                                class_mode=\"sparse\",\n",
        "                                shuffle=True)\n",
        "\n",
        "clases = [\"-\", \"BB\", \"BK\", \"BN\", \"BP\", \"BQ\", \"BR\", \"WB\", \"WK\", \"WN\", \"WP\", \"WQ\", \"WR\"]\n",
        "\n",
        "label_map = (validate.class_indices)\n",
        "\n",
        "print(label_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phOdPLMkqO8u",
        "colab_type": "text"
      },
      "source": [
        "**Start training:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VAsFbRFqLXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"3. Train model:\\n\")\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "history = model.fit(train, epochs=episodes,\n",
        "                    validation_data=validate,\n",
        "                    callbacks=[cp_callback])\n",
        "\n",
        "import numpy as np\n",
        "x = np.zeros((len(np.array(history.history['acc'])),4))\n",
        "\n",
        "x[:,0] = np.array(history.history['acc'])\n",
        "x[:,1] = np.array(history.history['val_acc'])\n",
        "x[:,2] = np.array(history.history['loss'])\n",
        "x[:,3] = np.array(history.history['val_loss'])\n",
        "\n",
        "np.savetxt(\"drive/My Drive/TFM/loss_history_vgg_set2_100epochs.txt\", x, delimiter=\",\")\n",
        "\n",
        "model.save('modeltrained.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voiMqhb6qUqK",
        "colab_type": "text"
      },
      "source": [
        "**Save results:** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vMYWU6N8gkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "x = np.zeros((len(np.array(history.history['acc'])),4))\n",
        "\n",
        "x[:,0] = np.array(history.history['acc'])\n",
        "x[:,1] = np.array(history.history['val_acc'])\n",
        "x[:,2] = np.array(history.history['loss'])\n",
        "x[:,3] = np.array(history.history['val_loss'])\n",
        "\n",
        "np.savetxt(\"loss_history.txt\", x, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXRkyYfJ2Hbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check training results\n",
        "\n",
        "#Plot training & validation accuracy values\n",
        "\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plot training & validation loss values\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09A05l3iqcua",
        "colab_type": "text"
      },
      "source": [
        "**Test Network without Data Augmentation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjeISv3A3kH5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bffa448a-acc2-4ba1-ab70-bccd4eb1e1c1"
      },
      "source": [
        "datagen_test = image.ImageDataGenerator(\n",
        "                                rescale=1./255,\n",
        "                                rotation_range=0,\n",
        "                                width_shift_range=0.00,\n",
        "                                height_shift_range=0.00)\n",
        "                                \n",
        "\n",
        "test = datagen_test.flow_from_directory(\n",
        "                                directory=test_folder,\n",
        "                                target_size=(224, 224),\n",
        "                                color_mode=\"rgb\",\n",
        "                                batch_size=64,\n",
        "                                class_mode=\"sparse\",\n",
        "                                shuffle=True,\n",
        "                                seed=42)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2353 images belonging to 13 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCPX-c69ndbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_acc = []\n",
        "\n",
        "\n",
        "for i in range(0,20):\n",
        "\n",
        "    test_img, test_labels = test.next()\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((test_img, test_labels))\n",
        "    test_dataset = test_dataset.batch(50)\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(test_dataset, verbose=2)\n",
        "    total_acc.append(test_acc)\n",
        "\n",
        "import statistics\n",
        "print(statistics.median(total_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnatVzYypfZf",
        "colab_type": "text"
      },
      "source": [
        "**Show prediction example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP8aKEScj4t3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "test_img, test_labels = test.next()\n",
        "pred = model.predict(test_img)\n",
        "ind = np.argmax(pred,axis=1)\n",
        "\n",
        "for i in range(0,8):\n",
        "  lbl = ind[i]\n",
        "  plt.subplot(2,4,i+1)\n",
        "  plt.imshow(test_img[i])\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.xlabel(clases[lbl])\n",
        "  plt.grid(False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}